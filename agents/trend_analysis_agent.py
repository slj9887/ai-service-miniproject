"""
TrendAnalysisAgent
ì„ ì •ëœ íŠ¸ë Œë“œì˜ ì—°êµ¬, ì‚°ì—…, ì‹œì¥, ë¦¬ìŠ¤í¬ ë“± ì •ë³´ë¥¼ RAG ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„
"""

import os
from typing import Dict
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from tavily import TavilyClient
from dotenv import load_dotenv
from agents.state_schema import SystemState, TrendAnalysis 
from utils.data_cleaner import clean_text

load_dotenv()
tavily = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
llm = ChatOpenAI(model="gpt-4o-mini")


def trend_analysis_agent(state: SystemState) -> SystemState:
    """
    TrendAnalysisAgent:
    1. Tavilyë¡œ íŠ¸ë Œë“œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
    2. ë²¡í„° ìŠ¤í† ì–´ë¡œ ì„ë² ë”©
    3. Retriever ê¸°ë°˜ RAG ë¶„ì„ ìˆ˜í–‰
    4. ë¶„ì„ ê²°ê³¼ë¥¼ state.trend_analysisì— ì €ì¥
    """

    trend = state.get("current_trend")
    if not trend:
        print("ë¶„ì„í•  íŠ¸ëœë“œ(current_trend)ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return state
    print(f"\n TrendAnalysisAgent: '{trend}' íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...")

    query = "Federated Learning technology 2026 OR research trends OR industrial applications OR challenges OR market forecast"
    response = tavily.search(query, max_results=20)

    docs = [
        Document(page_content=clean_text(r.get("content", "")[:1000]),
                metadata={"url": r.get("url")} )
        for r in response["results"]
    ]
    print(f"ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

    #ì„ë² ë”© + Chroma ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever(k=5)

    queries = {
        "definition": f"What is {trend} and why is it emerging?",
        "key_technologies": f"What are the core technologies or research areas driving {trend}?",
        "industry_trends": f"Which industries are adopting {trend}, and what are the notable use cases?",
        "adoption_flow": f"What stages of adoption are industries currently in for {trend}?",
        "future_outlook": f"What is the expected evolution or market forecast for {trend} by 2030?"
    }

    analysis: Dict[str, str]={}


    for topic, question in queries.items():
        retrieved_docs = retriever.invoke(question)
        combined_text = "\n\n".join([d.page_content for d in retrieved_docs])

        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ 2030ë…„ì„ ë‚´ë‹¤ë³´ëŠ” ë¯¸ë˜ ê¸°ìˆ  ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ '{trend}' íŠ¸ë Œë“œì˜ '{topic}'ì— ëŒ€í•´ ë¶„ì„ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
        5ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

        ==== ë¬¸ì„œ ====
        {context}
        """)

        chain = prompt | llm
        response = chain.invoke({"trend": trend, "topic": topic, "context": combined_text})
        analysis[topic] = response.content.strip()

        print(f"{topic} ë¶„ì„ ì™„ë£Œ")

    state["trend_analysis"] = analysis  
    state["vectorstore"] = vectorstore  

    print(f"\nğŸ“Š '{trend}' ë¶„ì„ ì™„ë£Œ! ({len(docs)}ê°œ ë¬¸ì„œ ê¸°ë°˜)")
    return state


if __name__ == "__main__":
    # âœ… í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
    state: SystemState = {"current_trend": "Federated Learning"}
    result = trend_analysis_agent(state)

    print("\nğŸ§© ìµœì¢… ë¶„ì„ ìš”ì•½:")
    for k, v in result["trend_analysis"].items():
        print(f"\n[{k.upper()}]\n{v}\n")